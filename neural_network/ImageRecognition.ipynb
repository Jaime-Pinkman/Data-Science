{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Implementation of Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "from scipy.optimize import fmin_cg\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load weights to verify cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat('ex4weights.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving parameters for the input and hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta1 = mat['Theta1']\n",
    "Theta2 = mat['Theta2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('ex4data1.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting X and y matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['X']\n",
    "y = data['y']\n",
    "\n",
    "# Shuffle them in random order (only rows)\n",
    "# to make model more flexible\n",
    "np.random.shuffle(X)\n",
    "np.random.shuffle(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset to train_set and test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_train = 4000\n",
    "m_test = 1000\n",
    "\n",
    "X_train = X[0:4000]\n",
    "y_train = y[0:4000]\n",
    "\n",
    "X_test = X[4000:]\n",
    "y_test = y[4000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unroll parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 401) 10025\n",
      "(10, 26) 260\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(Theta1), 25*401)\n",
    "print(np.shape(Theta2), 10*26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10285,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_params = np.concatenate([Theta1.flatten(), Theta2.flatten()])\n",
    "np.shape(nn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rollback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta1 = np.reshape(nn_params[0:10025], (-1, 401))\n",
    "Theta2 = np.reshape(nn_params[10025:10285], (-1, 26))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup some useful variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.shape(X)[0] # number of  examples\n",
    "n = np.shape(X)[1] # number of features\n",
    "k = 10 # number of labels\n",
    "J = 0 # Cost function value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting y( y(i) -> 1:10) to Y( Y(i) -> 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y = matrix of where each row(i) \n",
    "# represents corresponding X(i) example\n",
    "# and each column correlates to output's classes\n",
    "y[y == 0] = 10\n",
    "\n",
    "Y = np.zeros([m, k])\n",
    "for i in range(m):\n",
    "    j = y[i]\n",
    "    Y[i, j-1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    #return 1.0 / (1.0 + np.exp(-z))\n",
    "    return np.divide(1.0, (np.add(1.0, np.exp(np.negative(z)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-2fa504046c37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# The cost function value:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mlogErrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mJ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogErrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Y' is not defined"
     ]
    }
   ],
   "source": [
    "#predictions = sigmoid(np.ones([m, 1]), sigmoid([np.ones([])]))\n",
    "\n",
    "# Added bias feature to the input layer\n",
    "a1 = np.concatenate([np.ones([m, 1]), X], axis=1)\n",
    "\n",
    "# Activation of the hidden layer\n",
    "z2 = sigmoid(np.matmul(a1, np.transpose(Theta1)))\n",
    "      \n",
    "# Added bias feature to the hidden layer\n",
    "a2 = np.concatenate([np.ones([m, 1]), z2], axis=1)\n",
    "\n",
    "# Activation of the output layer\n",
    "z3 = sigmoid(np.matmul(a2, np.transpose(Theta2)))\n",
    "\n",
    "# The cost function value:\n",
    "logErrors = np.multiply(Y, np.log(z3)) + np.multiply(np.subtract(1, Y), np.log(np.subtract(1, z3)))\n",
    "J = -1/m * np.sum(logErrors)\n",
    "\n",
    "print(f\"The cost value without regularization is {J}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the Regularization term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logErrors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-28f635f9eba7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlmda\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTheta1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTheta2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mJ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogErrors\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"The cost value with regularization is {J}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logErrors' is not defined"
     ]
    }
   ],
   "source": [
    "# Choosing the lambda value\n",
    "lmda = 0.1\n",
    "\n",
    "# The reg term\n",
    "reg = lmda/(2*m) * (np.sum((Theta1[:, 1:]**2)) + np.sum((Theta2[:, 1:]**2)))\n",
    "\n",
    "J = -1/m * np.sum(logErrors) + reg\n",
    "print(f\"The cost value with regularization is {J}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the sigmoidGradient function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    return np.multiply(sigmoid(z), np.subtract(1, sigmoid(z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly initialize the parameters for symmetry breaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randInitializeWeights(L_in, L_out):\n",
    "    epsilon_init = np.sqrt(6)/(np.sqrt(L_in + L_out))\n",
    "    return np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10285,)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Added bias feature to the input layer\n",
    "a1 = np.concatenate([np.ones([m, 1]), X], axis=1)\n",
    "\n",
    "# Matrix Product of a1 and Theta1\n",
    "z2 = np.matmul(a1, np.transpose(Theta1))\n",
    "\n",
    "# Activation of the hidden layer\n",
    "a2 = np.concatenate([np.ones([m, 1]), sigmoid(z2)], axis=1)\n",
    "\n",
    "# Matrix Product of a2 and Theta2\n",
    "z3 = np.matmul(a2, np.transpose(Theta2))\n",
    "\n",
    "# Activation of the output layer\n",
    "a3 = sigmoid(z3)\n",
    "\n",
    "# Error of the output layer\n",
    "d3 = np.subtract(a3, Y)\n",
    "\n",
    "# Error of the hidden layer\n",
    "d2 = np.multiply(np.matmul(d3, Theta2[:, 1:]), sigmoidGradient(z2))\n",
    "\n",
    "# Accumulate the gradient, vectorized version\n",
    "Delta1 = np.matmul(np.transpose(d2), a1)\n",
    "Delta2 = np.matmul(np.transpose(d3), a2)\n",
    "\n",
    "# Obtain the gradient by dividing the accumulated gradients by m\n",
    "Theta1_grad = np.divide(Delta1, m)\n",
    "Theta2_grad = np.divide(Delta2, m)\n",
    "\n",
    "# Add regularization to the gradient\n",
    "Theta1_grad[:, 2:] = Theta1_grad[:, 2:] + lmda/m*Theta1_grad[:, 2:]\n",
    "Theta2_grad[:, 2:] = Theta2_grad[:, 2:] + lmda/m*Theta2_grad[:, 2:]\n",
    "\n",
    "# Unrolling out gradient parameters\n",
    "grad = np.concatenate([Theta1_grad.flatten(), Theta2_grad.flatten()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to find the cost value for scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(nn_params, X, y, lmda):\n",
    "    \n",
    "    X = np.reshape(X, (-1, 400))\n",
    "    \n",
    "    Theta1 = np.reshape(nn_params[0:10025], (-1, 401))\n",
    "    Theta2 = np.reshape(nn_params[10025:10285], (-1, 26))\n",
    "    \n",
    "    m = np.shape(X)[0]\n",
    "    y[y == 0] = 10\n",
    "    Y = np.zeros([m, k])\n",
    "    for i in range(m):\n",
    "        j = y[i]\n",
    "        Y[i, j-1] = 1\n",
    "\n",
    "    a1 = np.concatenate([np.ones([m, 1]), X], axis=1)\n",
    "\n",
    "    z2 = np.matmul(a1, np.transpose(Theta1))\n",
    "\n",
    "    a2 = np.concatenate([np.ones([m, 1]), sigmoid(z2)], axis=1)\n",
    "\n",
    "    z3 = np.matmul(a2, np.transpose(Theta2))\n",
    "\n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    logErrors = np.multiply(Y, np.log(a3)) + np.multiply(np.subtract(1, Y), np.log(np.subtract(1, a3)))\n",
    "\n",
    "    reg = lmda/(2*m) * (np.sum((Theta1[:, 1:]**2)) + np.sum((Theta2[:, 1:]**2)))\n",
    "\n",
    "    return -1/m * np.sum(logErrors) + reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to find the grad for scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backPropagate(nn_params, X, y, lmda):\n",
    "    \n",
    "    X = np.reshape(X, (-1, 400))\n",
    "    \n",
    "    Theta1 = np.reshape(nn_params[0:10025], (-1, 401))\n",
    "    Theta2 = np.reshape(nn_params[10025:10285], (-1, 26))\n",
    "    \n",
    "    m = np.shape(X)[0]\n",
    "    y[y == 0] = 10\n",
    "    Y = np.zeros([m, k])\n",
    "    for i in range(m):\n",
    "        j = y[i]\n",
    "        Y[i, j-1] = 1\n",
    "        \n",
    "    a1 = np.concatenate([np.ones([m, 1]), X], axis=1)\n",
    "\n",
    "    z2 = np.matmul(a1, np.transpose(Theta1))\n",
    "\n",
    "    a2 = np.concatenate([np.ones([m, 1]), sigmoid(z2)], axis=1)\n",
    "\n",
    "    z3 = np.matmul(a2, np.transpose(Theta2))\n",
    "\n",
    "    a3 = sigmoid(z3)\n",
    "\n",
    "    d3 = np.subtract(a3, Y)\n",
    "\n",
    "    d2 = np.multiply(np.matmul(d3, Theta2[:, 1:]), sigmoidGradient(z2))\n",
    "\n",
    "    Delta1 = np.matmul(np.transpose(d2), a1)\n",
    "    Delta2 = np.matmul(np.transpose(d3), a2)\n",
    "\n",
    "    Theta1_grad = np.divide(Delta1, m)\n",
    "    Theta2_grad = np.divide(Delta2, m)\n",
    "\n",
    "    Theta1_grad[:, 2:] = Theta1_grad[:, 2:] + lmda/m*Theta1_grad[:, 2:]\n",
    "    Theta2_grad[:, 2:] = Theta2_grad[:, 2:] + lmda/m*Theta2_grad[:, 2:]\n",
    "\n",
    "    return np.concatenate([Theta1_grad.flatten(), Theta2_grad.flatten()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scipy.optimize, fmin_cg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fmin_cg takes my cost_function, gradient_function, params and tries to optimize the cost value by iterating the gradient_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmin_cg_train():\n",
    "    randomThetas_unrolled = np.concatenate([randInitializeWeights(400, 25).flatten(), randInitializeWeights(25, 10).flatten()])\n",
    "    return fmin_cg(costFunction, fprime=backPropagate, x0=randomThetas_unrolled, args=(X_train.flatten(), y_train.flatten(), 0.1), maxiter=400, disp=True, full_output=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 2.316885\n",
      "         Iterations: 400\n",
      "         Function evaluations: 709\n",
      "         Gradient evaluations: 709\n"
     ]
    }
   ],
   "source": [
    "learned_Thetas = fmin_cg_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./nn_model.pkl', 'wb') as f:\n",
    "    pickle.dump(learned_Thetas, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./nn_model.pkl', 'rb') as f:\n",
    "    loaded_Thetas = pickle.load(f)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shaping Thetas\n",
    "Theta1 = np.reshape(loaded_Thetas[0:10025], (-1, 401))\n",
    "Theta2 = np.reshape(loaded_Thetas[10025:10285], (-1, 26))\n",
    "\n",
    "# Making a prediction\n",
    "a1 = np.concatenate([np.ones([m, 1]), X], axis=1)\n",
    "z2 = np.matmul(a1, np.transpose(Theta1))\n",
    "a2 = np.concatenate([np.ones([m, 1]), sigmoid(z2)], axis=1)\n",
    "z3 = np.matmul(a2, np.transpose(Theta2))\n",
    "a3 = sigmoid(z3)\n",
    "\n",
    "# Finding the indeces of the max value in each row\n",
    "predicted_values = np.argmax(a3, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_predict(learned_Thetas, X):\n",
    "    Theta1 = np.reshape(learned_Thetas[0:10025], (-1, 401))\n",
    "    Theta2 = np.reshape(learned_Thetas[10025:10285], (-1, 26))\n",
    "\n",
    "    m = np.shape(X)[0]\n",
    "    \n",
    "    a1 = np.concatenate([np.ones([m, 1]), X], axis=1)\n",
    "    z2 = np.matmul(a1, np.transpose(Theta1))\n",
    "    a2 = np.concatenate([np.ones([m, 1]), sigmoid(z2)], axis=1)\n",
    "    z3 = np.matmul(a2, np.transpose(Theta2))\n",
    "    a3 = sigmoid(z3)\n",
    "\n",
    "    return np.add(np.argmax(a3, axis=1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our model on the test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.6\n"
     ]
    }
   ],
   "source": [
    "# Making predictions\n",
    "predictions = nn_predict(loaded_Thetas, X_test)\n",
    "\n",
    "# Compare them to actual labels and get the accuracy\n",
    "print(np.mean((predictions == y_test.flatten()) * 100))\n",
    "\n",
    "#for i in range(1000):\n",
    "#    print(predictions[i], y_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I am working on accuracy :("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
