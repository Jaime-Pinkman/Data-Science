{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start with downloading necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import wrappers\n",
    "\n",
    "from numpy import *\n",
    "from numpy.random import uniform, normal\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from random import shuffle\n",
    "from collections import deque\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 2.0e-1           # the 'learning rate'\n",
    "maxEpisodes = 1000       # run the agent for 'maxEpisodes'\n",
    "maxTimeSteps = 500       # maximum number of steps per episode\n",
    "fixedNorm = 0.5          # output weights are scaled to have norm = 'fixedform'\n",
    "maxHistory = 5000        # maximum number of recent observations for replay\n",
    "solvedEpisodes = 100     # cartpole is solved when average reward > 195 for 'solvedEpisodes'\n",
    "episodeLength = 500      # the target for CartPole-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputLength = 4        # length of an observation vector\n",
    "expansionFactor = 30   # expand observation dimensions by 'expansionFactor'\n",
    "expandedLength = expansionFactor*inputLength # length of transformed observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature transform with fixed random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = normal(scale=1.0, size=(expandedLength, inputLength))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output weights, randomly initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = uniform(low=-1.0, high=1.0, size=expandedLength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix the norm of the output weights to 'fixedNorm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W *= fixedNorm/norm(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole NN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CartPoleAgent(aplha, W, V):\n",
    "    #--------------------------------------------\n",
    "    # Observation history\n",
    "    H = deque([], maxHistory)\n",
    "    # episode total reward history\n",
    "    R = deque([], solvedEpisodes)\n",
    "    # histories of positive and negative outputs\n",
    "    PO = deque([0], maxHistory)\n",
    "    NO = deque([0], maxHistory)\n",
    "    #--------------------------------------------\n",
    "    for episode in range(maxEpisodes):\n",
    "        observation = env.reset()\n",
    "        H.append(observation)\n",
    "        totalReward = 0\n",
    "        for t in range(1, maxTimeSteps + 1):\n",
    "            env.render()\n",
    "            #--------------------------------------------\n",
    "            out = dot(tanh(dot(V, observation)), W)\n",
    "            if out < 0:\n",
    "                NO.append(out)\n",
    "                action = 0\n",
    "            else:\n",
    "                PO.append(out)\n",
    "                action = 1\n",
    "            #--------------------------------------------\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            H.append(observation)\n",
    "            totalReward += reward\n",
    "            #--------------------------------------------\n",
    "            if done:\n",
    "                R.append(totalReward)\n",
    "                if t < episodeLength:\n",
    "                    #--------------------------------------------\n",
    "                    # Replay shuffled past observation using the\n",
    "                    # latest weights\n",
    "                    # Use the means of past outputs as\n",
    "                    # LMS algorithm target outputs\n",
    "                    #--------------------------------------------\n",
    "                    mn = mean(NO)\n",
    "                    mp = mean(PO)\n",
    "                    shuffle(H)\n",
    "                    for obs in H:\n",
    "                        h = tanh(dot(V, obs))     # transform the observation\n",
    "                        out = dot(h, W)\n",
    "                        if out < 0:\n",
    "                            e = mn - out\n",
    "                        else:\n",
    "                            e = mp - out\n",
    "                        W += aplha * e * h        # Widrow-Hoff LMS update\n",
    "                        W *= fixedNorm/norm(W)    # keep the weights at fixed norm\n",
    "                        #---------------------------------------------\n",
    "                    #--------------------------------------------------\n",
    "                    avgReward = sum(R)/solvedEpisodes\n",
    "                    print(f\"[{episode:3d}:{totalReward:3.0f}] R:{avgReward:6.2f} mp:{mean(PO):7.3f} mn:{mean(NO):7.3f}  len(H):{len(H):4d}  W:{W[:2]}\", flush=True)\n",
    "                    #---------------------------------------------\n",
    "                    if avgReward == episodeLength:\n",
    "                        print(\"Solved\")\n",
    "                        return\n",
    "                    #---------------------------------------------\n",
    "                    break\n",
    "                #---------------------------------------------\n",
    "            #---------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
