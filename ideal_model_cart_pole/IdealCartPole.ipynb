{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start with downloading necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import wrappers\n",
    "\n",
    "from numpy import *\n",
    "from numpy.random import uniform, normal\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from random import shuffle\n",
    "from collections import deque\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 2.0e-1           # the 'learning rate'\n",
    "maxEpisodes = 1000       # run the agent for 'maxEpisodes'\n",
    "maxTimeSteps = 500       # maximum number of steps per episode\n",
    "fixedNorm = 0.5          # output weights are scaled to have norm = 'fixedform'\n",
    "maxHistory = 5000        # maximum number of recent observations for replay\n",
    "solvedEpisodes = 100     # cartpole is solved when average reward > 195 for 'solvedEpisodes'\n",
    "episodeLength = 500      # the target for CartPole-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputLength = 4        # length of an observation vector\n",
    "expansionFactor = 30   # expand observation dimensions by 'expansionFactor'\n",
    "expandedLength = expansionFactor*inputLength # length of transformed observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature transform with fixed random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = normal(scale=1.0, size=(expandedLength, inputLength))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output weights, randomly initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = uniform(low=-1.0, high=1.0, size=expandedLength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix the norm of the output weights to 'fixedNorm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W *= fixedNorm/norm(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole NN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CartPoleAgent(aplha, W, V):\n",
    "    #--------------------------------------------\n",
    "    # Observation history\n",
    "    H = deque([], maxHistory)\n",
    "    # episode total reward history\n",
    "    R = deque([], solvedEpisodes)\n",
    "    # histories of positive and negative outputs\n",
    "    PO = deque([0], maxHistory)\n",
    "    NO = deque([0], maxHistory)\n",
    "    #--------------------------------------------\n",
    "    for episode in range(maxEpisodes):\n",
    "        observation = env.reset()\n",
    "        H.append(observation)\n",
    "        totalReward = 0\n",
    "        for t in range(1, maxTimeSteps + 1):\n",
    "            env.render()\n",
    "            #--------------------------------------------\n",
    "            out = dot(tanh(dot(V, observation)), W)\n",
    "            if out < 0:\n",
    "                NO.append(out)\n",
    "                action = 0\n",
    "            else:\n",
    "                PO.append(out)\n",
    "                action = 1\n",
    "            #--------------------------------------------\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            H.append(observation)\n",
    "            totalReward += reward\n",
    "            #--------------------------------------------\n",
    "            if done:\n",
    "                R.append(totalReward)\n",
    "                if t < episodeLength:\n",
    "                    #--------------------------------------------\n",
    "                    # Replay shuffled past observation using the\n",
    "                    # latest weights\n",
    "                    # Use the means of past outputs as\n",
    "                    # LMS algorithm target outputs\n",
    "                    #--------------------------------------------\n",
    "                    mn = mean(NO)\n",
    "                    mp = mean(PO)\n",
    "                    shuffle(H)\n",
    "                    for obs in H:\n",
    "                        h = tanh(dot(V, obs))     # transform the observation\n",
    "                        out = dot(h, W)\n",
    "                        if out < 0:\n",
    "                            e = mn - out\n",
    "                        else:\n",
    "                            e = mp - out\n",
    "                        W += aplha * e * h        # Widrow-Hoff LMS update\n",
    "                        W *= fixedNorm/norm(W)    # keep the weights at fixed norm\n",
    "                    #---------------------------------------------\n",
    "                #--------------------------------------------------\n",
    "                avgReward = sum(R)/solvedEpisodes\n",
    "                print(f\"[{episode:3d}:{totalReward:3.0f}] R:{avgReward:6.2f} mp:{mean(PO):7.3f} mn:{mean(NO):7.3f}  len(H):{len(H):4d}  W:{W[:2]}\", flush=True)\n",
    "                #---------------------------------------------\n",
    "                if avgReward == episodeLength:\n",
    "                    print(\"Solved\")\n",
    "                    return\n",
    "                #---------------------------------------------\n",
    "                break\n",
    "        #---------------------------------------------\n",
    "    #---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0: 49] R:  0.49 mp:  0.416 mn: -0.176  len(H):  50  W:[ 0.12690107 -0.04083459]\n",
      "[  1:101] R:  1.50 mp:  0.253 mn: -0.142  len(H): 152  W:[ 0.08298972 -0.02048573]\n",
      "[  2: 10] R:  1.60 mp:  0.253 mn: -0.247  len(H): 163  W:[ 0.03370082 -0.00082018]\n",
      "[  3:105] R:  2.65 mp:  0.546 mn: -0.418  len(H): 269  W:[-0.02553592 -0.03031049]\n",
      "[  4: 61] R:  3.26 mp:  0.530 mn: -0.366  len(H): 331  W:[0.10380925 0.00873646]\n",
      "[  5: 41] R:  3.67 mp:  0.502 mn: -0.355  len(H): 373  W:[ 0.1075776 -0.0419355]\n",
      "[  6: 47] R:  4.14 mp:  0.457 mn: -0.329  len(H): 421  W:[ 0.03350791 -0.0431425 ]\n",
      "[  7:  9] R:  4.23 mp:  0.457 mn: -0.385  len(H): 431  W:[-0.02391491  0.02926472]\n",
      "[  8: 10] R:  4.33 mp:  0.457 mn: -0.515  len(H): 442  W:[-0.01296161 -0.02240454]\n",
      "[  9:  9] R:  4.42 mp:  0.457 mn: -0.531  len(H): 452  W:[ 0.07488064 -0.04111523]\n",
      "[ 10: 89] R:  5.31 mp:  0.453 mn: -0.521  len(H): 542  W:[-0.09699483  0.01210728]\n",
      "[ 11: 15] R:  5.46 mp:  0.442 mn: -0.511  len(H): 558  W:[-0.03255088  0.05810095]\n",
      "[ 12:103] R:  6.49 mp:  0.414 mn: -0.464  len(H): 662  W:[0.07196545 0.03060397]\n",
      "[ 13: 29] R:  6.78 mp:  0.403 mn: -0.457  len(H): 692  W:[ 0.06448844 -0.05410122]\n",
      "[ 14: 36] R:  7.14 mp:  0.402 mn: -0.460  len(H): 729  W:[-0.06830404  0.00869637]\n",
      "[ 15:  9] R:  7.23 mp:  0.402 mn: -0.485  len(H): 739  W:[-0.0254571   0.04175546]\n",
      "[ 16:  9] R:  7.32 mp:  0.463 mn: -0.485  len(H): 749  W:[0.01125261 0.07009032]\n",
      "[ 17:  9] R:  7.41 mp:  0.492 mn: -0.485  len(H): 759  W:[0.09592897 0.01408787]\n",
      "[ 18: 32] R:  7.73 mp:  0.486 mn: -0.473  len(H): 792  W:[-0.0321043   0.02005404]\n",
      "[ 19: 10] R:  7.83 mp:  0.563 mn: -0.473  len(H): 803  W:[-0.06824808  0.03791543]\n",
      "[ 20:  9] R:  7.92 mp:  0.618 mn: -0.473  len(H): 813  W:[ 0.06372752 -0.00580996]\n",
      "[ 21:  9] R:  8.01 mp:  0.618 mn: -0.471  len(H): 823  W:[-0.08040377  0.01349389]\n",
      "[ 22:  8] R:  8.09 mp:  0.618 mn: -0.465  len(H): 832  W:[-0.06609725 -0.00920884]\n",
      "[ 23:  9] R:  8.18 mp:  0.618 mn: -0.521  len(H): 842  W:[-0.08322707  0.01957903]\n",
      "[ 24: 18] R:  8.36 mp:  0.606 mn: -0.511  len(H): 861  W:[-0.01626036  0.01166016]\n",
      "[ 25:  8] R:  8.44 mp:  0.606 mn: -0.558  len(H): 870  W:[-0.05184747 -0.02097254]\n",
      "[ 26: 10] R:  8.54 mp:  0.606 mn: -0.609  len(H): 881  W:[-0.05995337  0.01247256]\n",
      "[ 27:  9] R:  8.63 mp:  0.606 mn: -0.663  len(H): 891  W:[ 0.0287083  -0.05609544]\n",
      "[ 28: 10] R:  8.73 mp:  0.648 mn: -0.663  len(H): 902  W:[-0.03549521 -0.05001415]\n",
      "[ 29:500] R: 13.73 mp:  0.491 mn: -0.509  len(H):1403  W:[-0.03549521 -0.05001415]\n"
     ]
    }
   ],
   "source": [
    "CartPoleAgent(alpha, W, V)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
